
Main
----
Unit test all functionality.
Database.rb -> Change urls to return all urls and create urls_crawled and 
urls_uncrawled.
Put all classes in a single module called Pinch e.g. Pinch::Url etc. - See 
MiniTest documentation for a similar example.  This prevents naming conflicts. 
Use Utils#each where possible across project.
Use super if possible when overriding getter and setters.
Check for uses of @var instead of var in classes.
Append '.freeze' to all constants (removes warnings) and see if load.rb breaks.
Create a RoR app for the search engine front end.
Create an executable with some basic CLI func.

Other
-----
Sort tab formatting to be consistenent throughout (with 2 space tabs).
Think about ignoring non html documents/urls e.g. http://server/image.jpg etc.
Check if Document::TEXT_ELEMENTS is expansive enough.
Think about how we store DB connection details, is it right?
Add logging functionality (possibly use the mongo logger).
Think about potentially updating the crawler (main.rb) to be multi threaded.

Gem Publishing
--------------
Remove the DB details and ensure 'attr_reader :client' is commented out.
Check all tests are passing with minimal STDOUT noise.
Run 'yard' to update documentation.
Build gem from gemspec, install and test it manually.
Commit, tag and push it to the Git origin remote.
Upload the gem to rubygems.org.

Issues
------
If you crawl a base url and there is a link to its index file a duplicate doc
gets crawled and inserted into the DB because the base url and base/index point
to the same document.  One solution is to insert the doc html into the DB and
setup a unique index on the html field so the duplication will be picked up 
during the insert and the second doc won't be inserted.  This is potentially 
inefficient as the duplicate url is still crawled but at least the doc won't
be saved to the DB and therefore won't result in a duplicate search result.
The disadvantage is that you'd have to store the html which can take up space.
